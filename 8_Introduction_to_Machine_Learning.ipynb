{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T08:54:08.122004Z",
     "start_time": "2020-03-17T08:54:08.087696Z"
    }
   },
   "source": [
    "# A Machine Learning Case Study : The Boston Housing Dataset\n",
    "\n",
    "The algorithm used depends on whether the machine learning system is trained under human supervision. \n",
    "\n",
    "Machine Learning systems can be classified according to the amount and type of supervision they get during training. \n",
    "There are four major categories: \n",
    "- Supervised learning \n",
    "- Unsupervised learning\n",
    "- Semi-supervised learning\n",
    "- Reinforcement learning\n",
    "\n",
    "In the following session, we will go through a **real-life example** with **basic principles** of a **popular algorithm**.\n",
    "\n",
    "In supervised learning, the training data you feed the algorithm includes the desired solutions, called labels.\n",
    "In maths language, there is a corresponding y (solution) to every data point (x), and supervised learning is to learn how to best represent y in terms of x.\n",
    "\n",
    "In data science language, we usally call **y our target variable, and x our feature**.\n",
    "\n",
    "There are 2 sub-categories of supervised learning:\n",
    "- Classification\n",
    "- Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston Housing Dataset is derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n",
    "\n",
    "- **CRIM**: Per capita crime rate by town\n",
    "- **ZN**: Proportion of residential land zoned for lots over 25,000 sq. ft\n",
    "- **INDUS**: Proportion of non-retail business acres per town\n",
    "- **CHAS**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "- **NOX**: Nitric oxide concentration (parts per 10 million)\n",
    "- **RM**: Average number of rooms per dwelling\n",
    "- **AGE**: Proportion of owner-occupied units built prior to 1940\n",
    "- **DIS**: Weighted distances to five Boston employment centers\n",
    "- **RAD**: Index of accessibility to radial highways\n",
    "- **TAX**: Full-value property tax rate per 10,000 dollars\n",
    "- **PTRATIO**: Pupil-teacher ratio by town\n",
    "- **B**: $1000(Bk — 0.63)²$, where Bk is the proportion of people of African American descent by town\n",
    "- **LSTAT**: Percentage of lower status of the population\n",
    "- **MEDV**: Median value of owner-occupied homes in $1000s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Q1] Let's say we want to use some of these features to predict MEDV.\n",
    "\n",
    "Can you identify which type of machine learning problem we would be solving?\n",
    "\n",
    "* Supervised/Unsupervised?\n",
    "* Classification/Regression?\n",
    "* Univariate/Multivariate?\n",
    "\n",
    "Can you think of some examples for the other types of problems, using this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the librairies you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:06:42.071636Z",
     "start_time": "2020-03-24T09:06:38.811850Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:06:42.093395Z",
     "start_time": "2020-03-24T09:06:42.073779Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"BostonHousing.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:06:42.112660Z",
     "start_time": "2020-03-24T09:06:42.095513Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T06:54:26.200387Z",
     "start_time": "2020-03-23T06:54:26.143178Z"
    }
   },
   "source": [
    "### [Q2] Describe the dataset with one command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:06:42.116973Z",
     "start_time": "2020-03-24T09:06:42.114851Z"
    }
   },
   "outputs": [],
   "source": [
    "# Please answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Q3] Plot the histogram of the target variable \"medv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:06:42.121375Z",
     "start_time": "2020-03-24T09:06:42.118994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Please answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Q4] Plot the correlation matrix between all the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Remember that a correlation matrix can be computed only on quantitative variables (e.g. numbers, not categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:06:42.125186Z",
     "start_time": "2020-03-24T09:06:42.123135Z"
    }
   },
   "outputs": [],
   "source": [
    "# Please answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the relationship between our target variable MEDV and the variables to which it is the most correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Q5] Use a scatter plot to visualize the relationship between MEDV and LSTAT and MEDV and RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:06:42.419395Z",
     "start_time": "2020-03-24T09:06:42.126746Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (11,11))\n",
    "plt.scatter(df['lstat'], df['medv'], marker='o')\n",
    "plt.xlabel(\"LSAT\")\n",
    "plt.ylabel(\"MEDV\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:06:42.425242Z",
     "start_time": "2020-03-24T09:06:42.423117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Please answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![What now](images/what_now.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a predictive model based on these two features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to build a regression model, we need to keep some testing data that will be unknown to the model. This will enable us to evaluate the performance of the model on new data. Here we train the model on 80% of the data and test it on the remaining 20%.\n",
    "\n",
    "**Question: what problems could you foresee if we train on 100% of the data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:10:06.822161Z",
     "start_time": "2020-03-24T09:10:06.818661Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df[['lstat', 'rm']]\n",
    "y = df['medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:10:07.112545Z",
     "start_time": "2020-03-24T09:10:07.108841Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:10:07.454630Z",
     "start_time": "2020-03-24T09:10:07.451058Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit-learn** is a well-known Python package implementing Machine Learning algorithms. Let's now use scikit-learn to train our simple linear regression model.\n",
    "A linear regression model calculates an equation that minimizes the distance between the observed value and the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:10:08.697174Z",
     "start_time": "2020-03-24T09:10:08.691021Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to test our model on the remaining 20% of data. The metrics used to evaluate the performance of the model depend on the type of model you trained and the business problem. These metrics enable you to measure on your test data - which is unseen data for your model - the error of your model compared to the reality.\n",
    "The most common metrics used to evaluate a regression model are :\n",
    "\n",
    "- The **Mean Absolute Error (MAE)** measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight. The lower the MAE, the better your model is. \n",
    "\n",
    "$$\\large{MAE = \\frac{1}{n}\\sum\\nolimits_{i=1}^{n}{|y_i - \\widehat{y}_i|}}$$\n",
    "\n",
    "- The **Root Mean Square error** (RMSE) also measures the average magnitude of the error. It’s the square root of the average of squared differences between prediction and actual observation. The lower the RMSE, the better your model is. \n",
    "\n",
    "$$\\large{RMSE = \\sqrt{\\frac{1}{n}\\sum\\nolimits_{i=1}^{n}{(y_i - \\widehat{y_i})^{2}}}}$$\n",
    "\n",
    "- The **R-squared (R2)** is the percentage of the response variable variation that is explained by a linear model. It is always between 0 and 100%, and our aim is to maximise this measure - the closer to 100% is the R-squred, the more observed variation can the model explain.\n",
    "\n",
    "$$\\large{R^2 = \\frac{Explained\\ variation}{Total\\ variation}}$$\n",
    "\n",
    "\n",
    "Let's look at these metrics on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:10:26.806959Z",
     "start_time": "2020-03-24T09:10:26.799812Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# model evaluation for testing set\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"The model performance for testing set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n",
    "print('MAE score is {}'.format(mae))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also visualize the coefficients of your regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:06:43.552625Z",
     "start_time": "2020-03-24T09:06:43.548157Z"
    }
   },
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:06:43.559360Z",
     "start_time": "2020-03-24T09:06:43.555392Z"
    }
   },
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model estimated on the training data can be described as below:\n",
    "\n",
    "$$MEDV = - 0.72(LSTAT) + 4.59(RM) + 2.73 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: MEDV is *Median value of owner-occupied homes in $1000s*.\n",
    "\n",
    "* How should we interpret the coefficient of -0.72 in front of **LSTAT**, or *Percentage of lower status of the population?*\n",
    "* How should we interpret the coefficient of 4.59 in front of **RM**, or *Average number of rooms per dwelling*?\n",
    "* How should we interpret the **intercept** of +2.73? What does this represent?\n",
    "* Suppose we have a neighbourhood with 10% lower-status population, where the houses have, on average, 4.5 rooms. What should we expect the median value of an owner-occupied home in this neighbourhood to be?\n",
    "* Does this number make sense? When was this dataset published? What was the average price of a house back then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T08:16:41.934466Z",
     "start_time": "2020-03-30T08:16:41.808870Z"
    }
   },
   "source": [
    "![easy](images/easy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A classification problem : the Adult Income dataset\n",
    "\n",
    "Another common use case in Supervised Learning is classification. Let's try one example now with the **Adult Income** dataset. This dataset was extracted from the 1994 Census database and is described by the following variables:\n",
    "\n",
    "- **age**: continuous.\n",
    "- **workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "- **fnlwgt**: continuous.\n",
    "- **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "- **education-num**: continuous.\n",
    "- **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "- **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "- **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "- **race**: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "- **sex**: Female, Male.\n",
    "- **capital-gain**: continuous.\n",
    "- **capital-loss**: continuous.\n",
    "- **hours-per-week**: continuous.\n",
    "- **native-country**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "- **income**: `>50K`, `<=50K`\n",
    "\n",
    "Prediction task is to determine whether a person makes over 50K a year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:50:55.884351Z",
     "start_time": "2020-03-30T07:50:55.765545Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"adult.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:50:55.902217Z",
     "start_time": "2020-03-30T07:50:55.886255Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, you must keep in mind that before training a model, you need to get the big picture of the dataset and obtain first insights on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:50:56.001456Z",
     "start_time": "2020-03-30T07:50:55.904281Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:50:56.015332Z",
     "start_time": "2020-03-30T07:50:56.003759Z"
    }
   },
   "outputs": [],
   "source": [
    "df['income'].value_counts()/len(df['income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:50:56.021142Z",
     "start_time": "2020-03-30T07:50:56.017621Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "continuous_features = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:50:56.550536Z",
     "start_time": "2020-03-30T07:50:56.023577Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "correlation_matrix = df.corr().round(2)\n",
    "sns.heatmap(data=correlation_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:51:09.229097Z",
     "start_time": "2020-03-30T07:50:56.552568Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good means to obtain insights about a categorical feature is to visualize its distribution thanks to a **violin plot**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:52:33.442469Z",
     "start_time": "2020-03-30T07:52:28.590047Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature in categorical_features:\n",
    "    plt.figure(figsize = (13,10))\n",
    "    ax = sns.violinplot(x=feature, y=\"hours.per.week\", hue=\"income\",\n",
    "                        data=df, palette=\"Set2\", split=True)\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways of extracting information from data and turning it into insights. Check out **seaborn**'s documentation for more ideas about data visualization : https://seaborn.pydata.org/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target variable is a categorical feature with two categories. This is called a **binary classification** problem. Before fitting a model to the data, you need to convert it to a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:53:15.301816Z",
     "start_time": "2020-03-30T07:53:15.285161Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['income'] == '<=50K', 'income'] = 0\n",
    "df.loc[df['income'] == '>50K', 'income'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Keep also in mind that for almost all classification models, you need to **one-hot encode** the categorical variables. As a reminder, here is what does one-hot encoding :\n",
    "> One-hot encoding converts each categorical value into a new column and assigns a 1 or 0 (True/False) value to each row.\n",
    "> * **Advantage**: \"neutral\" representation of the data (does not assign an order)\n",
    "> * **Disadvantage**: can **significantly increase** the number of columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:53:16.118969Z",
     "start_time": "2020-03-30T07:53:16.087578Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns = categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also always need to keep a sample of your data on which you won't train your model to be able to test the performance of your model on unknown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:53:16.917730Z",
     "start_time": "2020-03-30T07:53:16.898170Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop('income', axis = 1)\n",
    "y = df['income']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training the logistic regression <a name=\"Logistic_regression\"></a>\n",
    "\n",
    "Logistic regression is a generalized linear model which helps to model a **binary variable**,\n",
    "via an exponential function as a 'link function', taking continuous variables tied in a regression model, \n",
    "as illustrated in the following equations, with y the output variable and $x_0, x_1, ..., x_n$ the explanatory varibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y =  \\exp(x_{0} + \\alpha_{1} * x_{1}\\ + ... + \\alpha_{n} * x_{n})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:53:18.221159Z",
     "start_time": "2020-03-30T07:53:18.217676Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:53:19.169418Z",
     "start_time": "2020-03-30T07:53:19.018138Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Let's compute the logistic regression\n",
    "# Initiate your logistic model\n",
    "logit = LogisticRegression(penalty='l2', tol=0.0001, C=1.0)\n",
    "# Fit your logistic regression model to your train model\n",
    "logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of a logistic regression with the function **predict_proba** is a probability for each class 0 or 1. You can also use directly the function **predict** that returns the predicted class associated with the probability with a default threshold of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:53:20.708403Z",
     "start_time": "2020-03-30T07:53:20.695514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict labels on your Test set of independent variables\n",
    "pred_logit = logit.predict(X_test)\n",
    "# Predict probabilities on your Test set of independent variables\n",
    "proba_logit = logit.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:53:21.069394Z",
     "start_time": "2020-03-30T07:53:21.065857Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:53:21.386479Z",
     "start_time": "2020-03-30T07:53:21.382832Z"
    }
   },
   "outputs": [],
   "source": [
    "proba_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the coefficients of the logistic regression, you can visualize the importance of each feature in your model with a barplot. This is a good means to add some value to your analysis and interpret your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T07:53:47.860131Z",
     "start_time": "2020-03-30T07:53:45.278818Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "coefs = pd.DataFrame(logit.coef_.reshape((108, 1)), \n",
    "                         index = X_train.columns.tolist(), columns = ['Importance'])\n",
    "coefs = coefs.sort_values(by='Importance', ascending=True)\n",
    "coefs.plot(kind='barh', figsize=(20,18), color = 'blue')\n",
    "plt.xlabel('Variable coefficients')\n",
    "plt.title('Variable coefficients of the linear regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to evaluate your classification model is to compute the **confusion matrix**. A confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).\n",
    "\n",
    "![Confusion matrix](images/confusionMatrix.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T08:56:32.758759Z",
     "start_time": "2020-03-30T08:56:32.732507Z"
    }
   },
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "table = pd.crosstab(y_test, pred_logit)\n",
    "print(table)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Confusion Matrix in percentges\")\n",
    "table = pd.crosstab(y_test, pred_logit) / len(y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like for a regression problem, a classification can be assessed with specific metrics. The main one is the **ROC curve** and the **AUC - Area Under the Curve**. The closest to 1 is your AUC, the better is your classification model. You will have a specific session on this topic, but in the meantime, you can find more details about what a ROC curve is [here](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5) and on [Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T08:39:42.944820Z",
     "start_time": "2020-03-30T08:39:42.704122Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "#roc curve\n",
    "auc = roc_auc_score(y_test, pred_logit)\n",
    "print(\"AUC : \" + str(auc))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, proba_logit[:,1], pos_label=1)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
